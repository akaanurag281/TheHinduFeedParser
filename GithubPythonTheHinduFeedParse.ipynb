{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akaanurag281/TheHinduFeedParser/blob/main/GithubPythonTheHinduFeedParse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msf5Q43e_EGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0022c48-a3c0-49e5-efbf-73f6d49efde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yRor5SJ_dUM"
      },
      "outputs": [],
      "source": [
        "!pip install feedparser\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_KEY\"\n",
        "client = OpenAI()\n",
        "def generate_summarizer(\n",
        "    prompt\n",
        "):\n",
        "    res = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        max_tokens=256,\n",
        "        temperature=0.1,\n",
        "        top_p=0.1,\n",
        "        frequency_penalty=1,\n",
        "        messages=\n",
        "       [\n",
        "         {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant for text summarization.\",\n",
        "         },\n",
        "         {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"Summarize this {prompt}\",\n",
        "         },\n",
        "        ],\n",
        "    )\n",
        "    return res.choices[0].message.content\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   response = client.embeddings.create(input=[text], model=model)\n",
        "   return response.data[0].embedding\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "_p6Sos6D8KEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v845NoGGM2i"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "def get_news_content(url,is_ai):\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    news_content = response.text\n",
        "    soup = BeautifulSoup(news_content, 'html.parser')\n",
        "    content_element = soup.find('div', {'class': 'articlebodycontent', 'itemprop': 'articleBody'})\n",
        "    if content_element:\n",
        "      news_content = content_element.get_text()\n",
        "      if is_ai:\n",
        "        return generate_summarizer(extract_text_before_comments(news_content))\n",
        "      else:\n",
        "        return extract_text_before_comments(news_content)\n",
        "    else:\n",
        "      print(\"Content element not found\")\n",
        "  else:\n",
        "    print(\"Failed to fetch news content from the provided link\")\n",
        "\n",
        "def extract_text_before_comments(text):\n",
        "    # Define a pattern to find \"Comments\" in a contextually irrelevant position (case-insensitive)\n",
        "    pattern = re.compile(r\"comments\", re.IGNORECASE)\n",
        "    # Find the index where \"Comments\" appears in the text\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        # Extract text before the occurrence of \"Comments\"\n",
        "        filtered_text = text[:match.start()].strip()\n",
        "        return filtered_text\n",
        "    else:\n",
        "        return \"No 'Comments' found or improperly formatted text\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('PATH_OF_PYQ_WITH_EMBEDDING')\n",
        "df['embedding'] = df['embedding'].apply(eval).apply(np.array ) #to be used for Numpy calculation\n",
        "#df Show output"
      ],
      "metadata": {
        "id": "iH8wt-vqHWAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_news_from_feed(rss_url):\n",
        "    # Initialize empty lists to store data\n",
        "    titles = []\n",
        "    links = []\n",
        "    summaries = []\n",
        "    news_contents = []\n",
        "    today = datetime.now().date()\n",
        "    yesterday = today - timedelta(days=1)\n",
        "\n",
        "    # Parse the RSS feed\n",
        "    feed = feedparser.parse(rss_url)\n",
        "    # Extract and print news content\n",
        "    for entry in feed.entries:\n",
        "        published_date = datetime.strptime(entry.published, \"%a, %d %b %Y %H:%M:%S %z\").date()\n",
        "        #if published_date == yesterday or published_date == today:\n",
        "        if  published_date == today:\n",
        "            titles.append(entry.title)\n",
        "            links.append(entry.link)\n",
        "            summaries.append(entry.summary)\n",
        "            news_contents.append(get_news_content(entry.link, is_ai=False))\n",
        "    data = {\n",
        "    'Title': titles,\n",
        "    'Link': links,\n",
        "    'Summary': summaries,\n",
        "    'News Content': news_contents\n",
        "        }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "all_news_df = pd.DataFrame()\n",
        "# List of RSS feed URLs\n",
        "rss_feeds = [\n",
        "    'https://www.thehindu.com/sci-tech/science/feeder/default.rss',\n",
        "    'https://www.thehindu.com/business/agri-business/feeder/default.rss',\n",
        "    'https://www.thehindu.com/business/Industry/feeder/default.rss',\n",
        "    'https://www.thehindu.com/business/Economy/feeder/default.rss',\n",
        "    'https://www.thehindu.com/business/budget/feeder/default.rss',\n",
        "    'https://www.thehindu.com/news/national/feeder/default.rss',\n",
        "     'https://www.thehindu.com/news/international/feeder/default.rss',\n",
        "    '#https://www.thehindu.com/news/states/feeder/default.rss',\n",
        "    'https://www.thehindu.com/news/cities/feeder/default.rss',\n",
        "    'https://www.thehindu.com/opinion/columns/feeder/default.rss',\n",
        "    'https://www.thehindu.com/opinion/editorial/feeder/default.rss'\n",
        "    # Add more feed URLs here...\n",
        "]\n",
        "\n",
        "csv_file_path = '/content/sample_data/todays_epaper.csv'\n",
        "# Check if the file exists\n",
        "if not os.path.exists(csv_file_path):\n",
        "  # Fetch news from each RSS feed\n",
        "     for rss_url in rss_feeds:\n",
        "         print(f\"Fetching news from {rss_url}\")\n",
        "         today_news_df = fetch_news_from_feed(rss_url)\n",
        "         all_news_df = pd.concat([all_news_df, today_news_df], ignore_index=True)\n",
        "\n",
        "     all_news_df['Combined_Text'] = all_news_df['Title'] + '-' + all_news_df['Summary'].astype(str)\n",
        "     all_news_df['embedding'] = all_news_df['Combined_Text'].apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
        "     all_news_df.to_csv(csv_file_path)\n",
        "     print(\"File saved successfully.\")\n",
        "else:\n",
        "    print(\"File already exists. Skipping the read operation.\")\n",
        "    all_news_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "all_news_df['embedding'] = all_news_df['embedding'].apply(eval).apply(np.array )\n",
        "#print(all_news_df)\n"
      ],
      "metadata": {
        "id": "Sr6odpb61HOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c32a015-699e-42d2-8007-68d1ec413420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists. Skipping the read operation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding news relevant for syllabus and PYQ\n",
        "final_news = []\n",
        "for index, line in all_news_df.iterrows():\n",
        "     # Adjust cosine similarity 0.830 as per use case\n",
        "     filtered_df = df[df['embedding'].apply(lambda x: cosine_similarity(x, line['embedding']) > 0.830)]\n",
        "     if not filtered_df.empty:\n",
        "       final_news.append(line['Link'])\n",
        "\n",
        "final_news_df = pd.DataFrame(final_news, columns=['Link'])\n",
        "final_news_df.to_csv('/content/sample_data/relevant_news_link.csv')"
      ],
      "metadata": {
        "id": "11kN2AYxJOyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhPMhf3mPiuqE6HN+kcQjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}